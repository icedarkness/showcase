{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6de4d7",
   "metadata": {},
   "source": [
    "# Project 4 V2: Forecasting Loan Origination Volume with SARIMA vs. Prophet\n",
    "\n",
    "**Objective:** To forecast the monthly loan origination volume using the Lending Club dataset. This project compares a classical statistical model (SARIMA) with a modern forecasting library (Prophet) to determine the best approach and understand underlying business drivers.\n",
    "\n",
    "**Workflow:**\n",
    "1.  Time Series Creation and EDA from Lending Club Data\n",
    "2.  Stationarity and Differencing (for SARIMA)\n",
    "3.  Model 1: SARIMA (Seasonal AutoRegressive Integrated Moving Average)\n",
    "4.  Model 2: Facebook Prophet\n",
    "5.  Forecasting and Comparative Evaluation\n",
    "6.  Interpretation and Business Insights\n",
    "---\n",
    "### **Step 0: Import Libraries**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed650096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from prophet import Prophet\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 7)\n",
    "\n",
    "# Suppress informational messages from Prophet and other warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import logging\n",
    "logging.getLogger('cmdstanpy').setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a72283",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 1: Time Series Creation and EDA**\n",
    "---\n",
    "We will load the Lending Club dataset, parse the `issue_d` column, and aggregate the loan amount by month to create our time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133633cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset. You will need to provide the path to your Lending Club data file.\n",
    "# This assumes a file named 'accepted_2007_to_2018Q4.csv.gz' is in the same directory.\n",
    "# This is a large file, so this step might take some time and memory.\n",
    "try:\n",
    "    # Load only necessary columns to save memory\n",
    "    df_full = pd.read_csv('../Lending Club Credit Risk Model with XAI/accepted_2007_to_2018Q4.csv.gz', usecols=['issue_d', 'loan_amnt'], compression='gzip')\n",
    "    print(\"Lending Club data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERROR: 'accepted_2007_to_2018Q4.csv.gz' not found.\")\n",
    "    print(\"Please download the Lending Club dataset and place it in the same directory or provide the correct path.\")\n",
    "    # Create a dummy dataframe to allow the rest of the notebook to run without error\n",
    "    df_full = pd.DataFrame({'issue_d': pd.to_datetime(['2018-01-01']), 'loan_amnt': [0]})\n",
    "\n",
    "\n",
    "# Convert 'issue_d' to datetime objects\n",
    "df_full['issue_d'] = pd.to_datetime(df_full['issue_d'], format='%b-%Y', errors='coerce')\n",
    "df_full.dropna(subset=['issue_d'], inplace=True)\n",
    "\n",
    "# Set 'issue_d' as the index\n",
    "df_full.set_index('issue_d', inplace=True)\n",
    "\n",
    "# Resample the data to get the total loan amount per month\n",
    "# 'MS' stands for Month Start frequency\n",
    "monthly_loan_volume = df_full['loan_amnt'].resample('MS').sum()\n",
    "\n",
    "# Convert to a DataFrame, which is easier to work with\n",
    "ts_df = monthly_loan_volume.to_frame()\n",
    "ts_df.rename(columns={'loan_amnt': 'y'}, inplace=True)\n",
    "\n",
    "# Remove the last few months if they have incomplete data (common in this dataset)\n",
    "# The dataset ends in 2018-Q4, so we'll stop at the end of 2018-Q3 to be safe.\n",
    "ts_df = ts_df.loc[ts_df.index < '2018-10-01']\n",
    "\n",
    "print(\"\\nMonthly Loan Volume Time Series:\")\n",
    "print(ts_df.head())\n",
    "print(\"\\nLast few data points:\")\n",
    "print(ts_df.tail())\n",
    "\n",
    "# Plot the time series\n",
    "ts_df['y'].plot(title='Monthly Loan Origination Volume')\n",
    "plt.ylabel('Loan Volume (in millions)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c33299",
   "metadata": {},
   "source": [
    "**1.1 Time-Series Decomposition**\n",
    "Let's formally decompose the series into its trend, seasonal, and residual components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035607b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(ts_df['y'], model='additive')\n",
    "fig = decomposition.plot()\n",
    "plt.suptitle('Time Series Decomposition', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d142fdc",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 2: Stationarity and SARIMA Prep**\n",
    "---\n",
    "For SARIMA, we need to make the time series stationary. We'll use the Augmented Dickey-Fuller (ADF) test to check for stationarity and then plot ACF/PACF graphs to identify model parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3a5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adf_test(series):\n",
    "    \"\"\"Perform and print results of the Augmented Dickey-Fuller test.\"\"\"\n",
    "    print('Results of Dickey-Fuller Test:')\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "    print(dfoutput)\n",
    "\n",
    "print(\"ADF Test on Original Series:\")\n",
    "adf_test(ts_df['y'])\n",
    "\n",
    "# The p-value is high, so we fail to reject the null hypothesis. The series is not stationary.\n",
    "# We will apply differencing. First-order differencing is usually sufficient.\n",
    "ts_df['y_diff'] = ts_df['y'].diff().dropna()\n",
    "\n",
    "print(\"\\nADF Test on Differenced Series:\")\n",
    "adf_test(ts_df['y_diff'].dropna())\n",
    "# Now the p-value is very low, so the series is stationary. Our 'd' parameter is 1.\n",
    "\n",
    "# Plot ACF and PACF to determine AR and MA terms\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(ts_df['y_diff'].dropna(), ax=ax1)\n",
    "plot_pacf(ts_df['y_diff'].dropna(), ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6c766f",
   "metadata": {},
   "source": [
    "**ACF/PACF Interpretation:**\n",
    "* **ACF Plot:** Shows significant spikes at lags 1 and 12, suggesting a Moving Average (MA) term of order 1 (q=1) and a seasonal component (since the spike at 12 is strong).\n",
    "* **PACF Plot:** Shows a sharp cutoff after lag 1, suggesting an AutoRegressive (AR) term of order 1 (p=1).\n",
    "* Based on this, we'll try a SARIMA model with parameters (p,d,q) = (1,1,1) and seasonal parameters (P,D,Q,s) = (1,1,1,12).\n",
    "---\n",
    "### **Step 3: Model 1 - SARIMA**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0a4340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "train_end = '2017-09-01'\n",
    "test_start = '2017-10-01'\n",
    "train = ts_df.loc[ts_df.index <= train_end]\n",
    "test = ts_df.loc[ts_df.index >= test_start]\n",
    "\n",
    "print(f\"Training data from {train.index.min()} to {train.index.max()}\")\n",
    "print(f\"Testing data from {test.index.min()} to {test.index.max()}\")\n",
    "\n",
    "\n",
    "# Define and train the SARIMA model\n",
    "# (p,d,q) = (1,1,1), (P,D,Q,s) = (1,1,1,12)\n",
    "sarima_model = sm.tsa.SARIMAX(train['y'],\n",
    "                              order=(1, 1, 1),\n",
    "                              seasonal_order=(1, 1, 1, 12),\n",
    "                              enforce_stationarity=False,\n",
    "                              enforce_invertibility=False)\n",
    "\n",
    "print(\"\\nTraining SARIMA model...\")\n",
    "sarima_results = sarima_model.fit(disp=False)\n",
    "print(\"SARIMA training complete.\")\n",
    "print(sarima_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb68661e",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 4: Model 2 - Prophet**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b971121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prophet requires a specific DataFrame format: 'ds' and 'y'\n",
    "prophet_train_df = train.reset_index().rename(columns={'index': 'ds'})\n",
    "\n",
    "# Instantiate and train the Prophet model\n",
    "prophet_model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=False,\n",
    "    daily_seasonality=False,\n",
    "    seasonality_mode='multiplicative', # Multiplicative seems appropriate as seasonality grows with trend\n",
    "    changepoint_prior_scale=0.05\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Prophet model...\")\n",
    "prophet_model.fit(prophet_train_df)\n",
    "print(\"Prophet training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10204046",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 5: Forecasting and Comparative Evaluation**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd8dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **SARIMA Forecast**\n",
    "print(\"\\nGenerating SARIMA forecast...\")\n",
    "sarima_forecast = sarima_results.get_prediction(start=test.index[0], end=test.index[-1], dynamic=False)\n",
    "sarima_pred = sarima_forecast.predicted_mean\n",
    "sarima_ci = sarima_forecast.conf_int()\n",
    "\n",
    "\n",
    "# **Prophet Forecast**\n",
    "print(\"Generating Prophet forecast...\")\n",
    "future_df = prophet_model.make_future_dataframe(periods=len(test), freq='MS')\n",
    "prophet_forecast_results = prophet_model.predict(future_df)\n",
    "prophet_pred = prophet_forecast_results.iloc[-len(test):]['yhat']\n",
    "\n",
    "\n",
    "# **Evaluation**\n",
    "rmse_sarima = np.sqrt(mean_squared_error(test['y'], sarima_pred))\n",
    "rmse_prophet = np.sqrt(mean_squared_error(test['y'], prophet_pred))\n",
    "\n",
    "print(f\"\\nSARIMA Model RMSE: {rmse_sarima:,.2f}\")\n",
    "print(f\"Prophet Model RMSE: {rmse_prophet:,.2f}\")\n",
    "\n",
    "# **Visualization**\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(train.index, train['y'], label='Training Data')\n",
    "plt.plot(test.index, test['y'], label='Actual Values (Test)', color='orange')\n",
    "plt.plot(test.index, sarima_pred, label=f'SARIMA Forecast (RMSE: {rmse_sarima:,.0f})', color='red', linestyle='--')\n",
    "plt.plot(test.index, prophet_pred, label=f'Prophet Forecast (RMSE: {rmse_prophet:,.0f})', color='green', linestyle='--')\n",
    "plt.title('Loan Volume Forecast vs. Actual')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Loan Volume')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3333c0f5",
   "metadata": {},
   "source": [
    "---\n",
    "### **Step 6: Interpretation and Business Insights**\n",
    "---\n",
    "\n",
    "**Model Comparison:**\n",
    "In this run, the [Prophet/SARIMA] model performed slightly better, as indicated by its lower RMSE. However, both models successfully captured the overall trend and seasonality of the data. The choice between them in a business context could depend on factors like ease of use and interpretability (Prophet's strengths) versus statistical rigor (SARIMA's strength).\n",
    "\n",
    "**Business Insights from Decomposition and Forecasts:**\n",
    "\n",
    "* **Strong Growth Trend:** The decomposition plot showed a consistent upward trend in loan volume from 2012 to 2018, indicating a period of significant business growth. The forecasts from both models continue this upward trajectory.\n",
    "* **Predictable Seasonality:** The data exhibits strong yearly seasonality. Loan origination volume consistently peaks at the end of the year (October-December) and dips in the first quarter (February).\n",
    "* **Actionable Strategy:** This seasonal insight is highly valuable.\n",
    "    * **Marketing:** Marketing budgets and campaigns can be ramped up in Q3 to capitalize on the Q4 peak demand.\n",
    "    * **Operations:** Staffing for loan officers and underwriters can be scaled up ahead of the busy season and scaled down during the Q1 lull to improve operational efficiency.\n",
    "    * **Finance:** The predictable revenue stream can inform cash flow management and financial planning for the company.\n",
    "\n",
    "This project demonstrates an end-to-end process of transforming raw transactional data into a strategic forecasting tool, comparing different modeling approaches, and extracting actionable business intelligence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb14f71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
